---
title: "The Illusion of Control: What the Fall of the USSR Teaches Us About AI Governance"
date: 2026-02-12
permalink: /essays/the-illusion-of-control/
layout: single
author_profile: true
toc: true
toc_label: "Contents"
toc_icon: "file-text"
classes: wide
excerpt: "Institutions now claiming to govern artificial intelligence are replicating a pattern I have watched destroy more durable systems than themselves. They are performing control rather than exercising it."
tags:
  - AI Governance
  - Institutional Collapse
  - Systems Analysis
  - USSR
  - Emerging Markets
---

*I was twenty-two years old on August 31, 1991.*

That evening, my father came home from the Supreme Soviet session in Tashkent. My father — Doctor of Economics, People's Deputy, Chairman of the Committee on Economic Reforms — was one of those who directly participated in drafting the document. A document that, in a single session, dissolved what four generations of my family had been told: this is permanent, this is inevitable, this is indestructible. The Soviet Union — a nuclear superpower, the world's largest state, the system that had shaped every institution, every career, every assumption about how the world works — was ending.

That same evening we all sat in front of the television and watched President Karimov's address. The reactions in the room were different. Some did not understand what had happened. Others did not want to accept it. The world that had existed their entire lives had just ceased to exist — and they had no words for it.

What I remember most is not the celebration. It is the silence before it. For a moment, the deputies in that hall understood they had just done something irreversible. Not merely historical. Epochal.

And yet, just three months earlier, almost no one in that building — or anywhere else — had believed this was possible.

That is what I think about when I read AI governance frameworks today. Not the sophistication of the language. The silence before it.

---

**The thesis is simple:** the institutions now claiming to govern artificial intelligence are replicating a pattern I have watched destroy more durable systems than themselves. They are performing control rather than exercising it. And the mechanisms of that performance — the gap between what institutions declare and what they can actually enforce — are identical to the mechanisms I have witnessed in every major systemic collapse of the past thirty-five years.

---

## I. The Architecture of Declared Control

In the Soviet system, control was total — on paper. The state planned everything: production, prices, distribution, culture. Gosplan produced volumes of data, projections, and targets. Committees oversaw committees. Every institution reported upward through chains of supervision that, in theory, extended to the Politburo.

The enforcement gap — the space between declared authority and actual capacity — was invisible until it wasn't.

This is not ancient history. The same architecture exists today in AI governance. The EU AI Act runs to hundreds of pages of regulatory requirements \[1\]. OpenAI publishes safety frameworks and model cards. Anthropic commits to responsible scaling policies. Voluntary industry coalitions multiply.

The failure mechanism is identical: **performative control** — the institutional production of oversight appearances without the enforcement infrastructure to match.

As of early 2026, there is still no broadly empowered, independent auditing regime with routine access to frontier model internals across major labs \[2\]. Some oversight bodies exist, but their access remains limited, voluntary, or non-uniform. The transparency exists. The accountability does not.

*The lesson from 1991: when the gap between declared control and actual control becomes too wide, the system does not gradually weaken. It collapses suddenly. And the collapse surprises everyone — including those who built it.*

---

## II. The Board That Thought It Had Power

On November 17, 2023, the board of directors of OpenAI fired Sam Altman \[3\]. They had legal authority to do so. They exercised it.

Within five days, Altman was back. Employees threatened mass resignation. Microsoft — OpenAI's largest investor with $13 billion committed — made clear that the nonprofit mission the board was protecting was secondary to the capital already at stake.

When I read about this, my first thought was: history repeats itself. In 1985, Steve Jobs was removed from Apple. The board had formal authority. The institution had other centers of gravity. But my second thought was more disturbing.

The OpenAI board believed it was steering the organization developing frontier AI. It was not. It discovered that the center of gravity had moved elsewhere. The real decisions — about safety trade-offs, deployment timelines, which risks were acceptable — were being made at the intersection of competitive pressure, investor expectations, and technical momentum.

The mechanism here is **incentive misalignment at institutional scale.** The board's formal authority and the actual locus of decision-making had separated. This separation was invisible until it mattered.

I have seen this before — not in tech, but in state institutions facing market pressures they were not designed to manage. The structure remains. The control evaporates. The structure holds a little longer. Then something breaks.

---

## III. The 2008 Rehearsal

In 2008, the financial system demonstrated what happens when risk management becomes a performance for regulators rather than a genuine operational constraint \[4\].

Rating agencies declared mortgage-backed securities safe. Banks declared their exposure managed. Regulators declared oversight adequate. All of these declarations were backed by models, frameworks, and documentation.

The mechanism: **information asymmetry weaponized through complexity.** Those who created the instruments knew more about their actual risk than those meant to oversee them. Transparency produced data. It did not produce accountability.

AI governance is beginning to replicate the same mechanism. Labs publish model cards. They disclose training data characteristics. They report on red-teaming exercises. But the actual risk profiles of frontier models — emergent capabilities, potential misuse vectors, interaction effects at scale — remain inaccessible to any external auditor. Not because they are hidden, but because the technical capacity to evaluate them does not yet exist in any regulatory body.

*We have seen this pattern before — and we know what the failure modes look like.*

---

## IV. The Coordination Mirage

Climate governance has spent thirty years demonstrating the limits of voluntary frameworks for collective action problems \[5\].

The COP process has produced agreements, targets, pledges, and communiqués since 1995. Global emissions have continued to rise. The failure mechanism is not bad faith. The mechanism is **national interest asymmetry**: the costs of compliance are immediate and local; the benefits of collective action are deferred and diffuse.

AI governance is structurally identical. The voluntary commitments signed at the Bletchley Park AI Safety Summit (2023) and Seoul AI Safety Summit (2024) follow the same logic as COP pledges: credible at signing, eroding under competitive pressure \[6\].

The window for governance frameworks to catch up is not thirty years. Not even three years. **Possibly months.**

---

## Implications

**First:** the current wave of AI governance activity is producing the appearance of oversight faster than its substance. The gap between declared control and actual control is widening, not narrowing.

**Second:** the next major AI governance failure is likely to be sudden rather than gradual. Like the OpenAI board crisis, like the 2008 collapse, like August 1991 — it will surprise people who had access to all the relevant information but were reading the wrong signals.

**Third:** institutions outside the current governance conversation — insurance underwriters, institutional investors, government procurement officers, trade finance regulators — may impose more effective constraints on AI development than any voluntary safety framework. This is how systemic risk has historically been disciplined: not by the institutions that created it, but by the institutions that price it.

---

## Signals to Watch

- **Safety team / deployment team ratio** at frontier labs. Declining ratio = widening enforcement gap, regardless of public commitments.
- **Progress of AI liability legislation** in any major jurisdiction. Voluntary commitments without liability exposure are structurally identical to Soviet production targets without consequences for non-compliance.
- **Behavioral divergence** between AI companies' insurers/investors and their public positioning. When those who price tail risk act differently from those managing public messaging, the enforcement gap has become visible to people with real stakes.
- **Frequency and severity of publicly disclosed AI incidents** — and whether incident reporting moves from voluntary to mandatory in any major jurisdiction.

---

## The Questions That Remain Open

My father's generation believed — until they could not — that the system they had built was stable because it had always been stable.

I am not predicting collapse. I am identifying a pattern. The pattern is: institutions claim control they do not have. They build documentation architectures that produce the appearance of accountability. They are surprised when the gap between declaration and reality closes violently rather than gradually.

The question is not whether AI governance will face a moment of reckoning. The question is whether that moment will occur when there is still time to correct course.

No one in that hall in Tashkent on August 31, 1991, believed three months earlier that this day would come. **No one believed it — and no one believes it now.**

**What are you certain about today — and what will you no longer be certain about in three months?**

*My father — today a Professor and Academician at Samarkand Institute of Economics and Statistics — continues to work. He witnessed the end of one system. He is watching the beginning of another.*

---

## Sources & Notes

\[1\] European Parliament. *Regulation (EU) 2024/1689 — Artificial Intelligence Act.* Official Journal of the European Union, 12 July 2024. [eur-lex.europa.eu](https://eur-lex.europa.eu)

\[2\] UK AI Safety Institute; US AI Safety Institute (NIST). As of early 2026, no jurisdiction has established mandatory independent auditing with routine access to frontier model weights or training processes across major labs.

\[3\] Removal of Sam Altman from OpenAI, November 2023. Multiple public reporting: New York Times; CNN Business; Fortune; NPR; Financial Times. Microsoft's $13B investment confirmed across multiple sources. Altman reinstated November 22, 2023.

\[4\] Financial Crisis Inquiry Commission. *The Financial Crisis Inquiry Report.* U.S. Government Publishing Office, 2011. [fcic.law.stanford.edu](https://fcic.law.stanford.edu)

\[5\] IPCC. *Synthesis Report of the IPCC Sixth Assessment Report (AR6).* Geneva, 2023. [ipcc.ch](https://ipcc.ch)

\[6\] Bletchley Declaration on AI Safety, November 2023 (28 countries). Seoul Ministerial Statement for Advancing AI Safety, May 2024. Available via UK Department for Science, Innovation and Technology. [gov.uk](https://gov.uk)

\[7\] Uzbekistan Declaration of Independence. Supreme Council of the Uzbek Soviet Socialist Republic, August 31, 1991. September 1 designated Independence Day by Law of the Republic of Uzbekistan.

\[8\] For the governance/control gap in AI, see also: CSET (Georgetown). *AI Safety Technical Research* series 2023–2025; Anthropic. *Responsible Scaling Policy*, September 2023. [anthropic.com](https://anthropic.com)

---

*Oybek Khodjaev is a systems transformation analyst and Founder & CEO of INVEXI LLC. In 2019–2022 he served as Deputy Khokim of Samarkand Region. He brings nearly thirty years of experience in economics, banking, finance, and business in Uzbekistan and the CIS.*

*For comments, questions, or a briefing request: [ok@okhodjaev.com](mailto:ok@okhodjaev.com)*

